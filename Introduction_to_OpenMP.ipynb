{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7rc1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Praveen76/Introduction-to-OpenMP/blob/main/Introduction_to_OpenMP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lesbian-plaza"
      },
      "source": [
        "## Learning Objectives"
      ],
      "id": "lesbian-plaza"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "identical-object"
      },
      "source": [
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* understand the parallelization in python\n",
        "* implement multiprocessing using OpenMP"
      ],
      "id": "identical-object"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "communist-baghdad"
      },
      "source": [
        "## Information"
      ],
      "id": "communist-baghdad"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "large-terrorist"
      },
      "source": [
        "OpenMP is an Application Program Interface (API), jointly defined by a group of major computer hardware and software vendors. It provides a portable, scalable model for developers of shared memory parallel applications\n",
        "\n",
        "OpenMP is the easiest to use and requires the minimum learning overhead and  most key parallel design patterns can be learned with OpenMP.\n",
        "\n",
        "**Concurrency:** A condition of a system in which multiple\n",
        "tasks are logically active at one time.\n",
        "\n",
        "**Parallelism:** A condition of a system in which multiple\n",
        "tasks are actually active at one time.\n",
        "\n",
        "![link text](https://cdn.iisc.talentsprint.com/CDS/Images/OpenMP_Concurrent_Parallel.JPG)"
      ],
      "id": "large-terrorist"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Steps:"
      ],
      "metadata": {
        "id": "rKQ0Fvl_jNqU"
      },
      "id": "rKQ0Fvl_jNqU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beneficial-alexander"
      },
      "source": [
        "#### Importing required packages"
      ],
      "id": "beneficial-alexander"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "critical-telephone"
      },
      "source": [
        "import concurrent.futures\n",
        "# The concurrent.futures module provides a high-level interface for asynchronously executing callables.\n",
        "import time\n",
        "from multiprocessing import Pool\n",
        "# Pool is for parallelizing the execution of a function across multiple input values\n",
        "import numpy as np"
      ],
      "id": "critical-telephone",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "metropolitan-villa"
      },
      "source": [
        "Let us derive a function that performs an action with sleep time and execute it with concurrent pool executors"
      ],
      "id": "metropolitan-villa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "approved-retrieval"
      },
      "source": [
        "start = time.perf_counter() # return time as nanoseconds\n",
        "\n",
        "# function to delay the execution of a program\n",
        "def do_something(seconds):\n",
        "    print('Sleeping {} second(s)...'.format(seconds))\n",
        "    time.sleep(seconds)\n",
        "    return f'Done Sleeping...{seconds}'\n",
        "\n",
        "# chops iterables into a number of chunks which it submits to the pool as separate tasks.\n",
        "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
        "    secs = [5, 4, 3, 2, 1]\n",
        "    results = executor.map(do_something, secs)\n",
        "\n",
        "finish = time.perf_counter()\n",
        "print('Finished in {} second(s)'.format(finish-start))"
      ],
      "id": "approved-retrieval",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "turkish-christopher"
      },
      "source": [
        "### Parallelization in Python"
      ],
      "id": "turkish-christopher"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "collected-point"
      },
      "source": [
        "Python does not thread very well. Specifically, Python has a very nasty drawback known as a Global Interpreter Lock (GIL). The GIL ensures that only one compute thread can run at a time. This makes multithreaded processing very difficult. Instead, the best way to go about doing things is to use multiple independent processes to perform the computations. This method skips the GIL, as each individual process has it’s own GIL that does not block the others. This is typically done using the multiprocessing module."
      ],
      "id": "collected-point"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "missing-tribune"
      },
      "source": [
        "The pool object gives us a set of parallel workers we can use to parallelize our calculations. In particular, there is a map function (with identical syntax to the map() function used earlier), that runs a workflow in parallel.\n",
        "\n",
        "Let’s try map() out with a test function that just runs sleep."
      ],
      "id": "missing-tribune"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "familiar-occasion"
      },
      "source": [
        "# function for time sleep with 0.1 sec\n",
        "def sleeping(arg):\n",
        "    time.sleep(0.1)\n",
        "\n",
        "%timeit list(map(sleeping, range(24)))"
      ],
      "id": "familiar-occasion",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visible-campus"
      },
      "source": [
        "%timeit is an ipython magic function, which can be used to time a particular piece of code (A single execution statement, or a single method).\n",
        "\n",
        "To know more about `%timeit` click [here](https://ipython.org/ipython-doc/dev/interactive/magics.html#magic-timeit)"
      ],
      "id": "visible-campus"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "consecutive-competition"
      },
      "source": [
        "Now let’s try it in parallel:"
      ],
      "id": "consecutive-competition"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ynqc2e_dGZMU"
      },
      "source": [
        "pool = Pool(4)"
      ],
      "id": "Ynqc2e_dGZMU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "processed-delicious"
      },
      "source": [
        "%timeit pool.map(sleeping, range(24))"
      ],
      "id": "processed-delicious",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conditional-knock"
      },
      "source": [
        "The multiprocessing module has a major limitation: it only accepts certain  functions, and in certain situations. For instance any class methods, lambdas, or functions defined in __main__ won't work. This is due to the way Python “pickles” (read: serializes) data and sends it to the worker processes. “Pickling” simply can’t handle a lot of different types of Python objects.\n",
        "\n",
        "Fortunately, there is a fork of the multiprocessing module called *multiprocess* that works just fine. *multiprocess* uses dill instead of pickle to serialize Python objects (read: send your data and functions to the Python workers), and does not suffer the same issues. Usage is identical:"
      ],
      "id": "conditional-knock"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install multiprocess\n",
        "from multiprocess import Pool"
      ],
      "metadata": {
        "id": "LaKO3FDcKj3J"
      },
      "id": "LaKO3FDcKj3J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "capable-grade"
      },
      "source": [
        "# shut down the old workers\n",
        "pool.close()\n",
        "\n",
        "pool = Pool(4)\n",
        "%timeit pool.map(lambda x: time.sleep(0.1), range(24))\n",
        "pool.close()"
      ],
      "id": "capable-grade",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "running-regression"
      },
      "source": [
        "This is a general purpose parallelization recipe that we can use for your Python projects."
      ],
      "id": "running-regression"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lightweight-representative"
      },
      "source": [
        "# function to square the number\n",
        "def square(x):\n",
        "    return x**2"
      ],
      "id": "lightweight-representative",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amber-commodity"
      },
      "source": [
        "# make sure to always use multiprocess\n",
        "\n",
        "number_of_cores = 4\n",
        "# start your parallel workers at the beginning of your script\n",
        "pool = Pool(number_of_cores)\n",
        "\n",
        "start = time.perf_counter()\n",
        "\n",
        "# execute a computation(s) in parallel\n",
        "result = pool.map(square, range(24))\n",
        "result2 = pool.map(square, range(24))\n",
        "\n",
        "finish = time.perf_counter()\n",
        "print(f'Finished in {round(finish-start, 2)} second(s)')\n",
        "\n",
        "# turn off your parallel workers at the end of your script\n",
        "pool.close()"
      ],
      "id": "amber-commodity",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stunning-fighter"
      },
      "source": [
        "### MultiProcessing in Python using openMP"
      ],
      "id": "stunning-fighter"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "advisory-patent"
      },
      "source": [
        "#### OpenMP\n",
        "OpenMP employs a few principles in its programming model. The first is that everything takes place in threads. The second is the fork-join model, which comprises parallel regions in which one or more threads can be used\n",
        "\n",
        "![link text](https://cdn.iisc.talentsprint.com/CDS/Images/Fork_join.png)\n",
        "\n",
        "Above figure depicts the illustration of the fork–join paradigm, in which three regions of the program permit parallel execution of the variously colored blocks. Sequential execution is displayed on the top, while its equivalent fork–join execution is on the bottom."
      ],
      "id": "advisory-patent"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "increased-fundamental"
      },
      "source": [
        "#### Pymp\n",
        "Because the goal of Pymp is to bring OpenMP-like functionality to Python, Pymp and Python should naturally share some concepts. A single master thread forks into multiple threads, sharing data and then synchronizing (joining) and destroying the threads.\n",
        "\n",
        "As with OpenMP applications, when Pymp Python code hits a parallel region, processes – termed child processes – are forked and are in a state that is nearly the same as the “master process.” Note that these are forked processes and not threads, as is typical with OpenMP applications. As for the shared memory, according to the Pymp website, “… the memory is not copied, but referenced. Only when a process writes into a part of the memory [does] it gets its own copy of the corresponding memory region. This keeps the processing overhead low (but of course not as low as for OpenMP threads).”"
      ],
      "id": "increased-fundamental"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "educational-interim"
      },
      "source": [
        "# install the pymp\n",
        "!pip -qq install pymp-pypi"
      ],
      "id": "educational-interim",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "demonstrated-chicken"
      },
      "source": [
        "To keep things simple, this is a serial code with a single array."
      ],
      "id": "demonstrated-chicken"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "listed-basement"
      },
      "source": [
        "# creating an array of zeros\n",
        "ex_array = np.zeros((100,), dtype='uint8')\n",
        "for index in range(0, 100):\n",
        "    # assigning 1\n",
        "    ex_array[index] = 1\n",
        "    print('Yay! {} done!'.format(index))"
      ],
      "id": "listed-basement",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "universal-royalty"
      },
      "source": [
        "Let's start with Pymp version of the same code by importing the pymp"
      ],
      "id": "universal-royalty"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "professional-liquid"
      },
      "source": [
        "import pymp"
      ],
      "id": "professional-liquid",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "contemporary-quarter"
      },
      "source": [
        "ex_array = pymp.shared.array((100,), dtype='uint8')\n",
        "with pymp.Parallel(4) as p:\n",
        "    for index in p.range(0, 100):\n",
        "        ex_array[index] = 1\n",
        "        # The parallel print function takes care of asynchronous output.\n",
        "        p.print('Yay! {} done!'.format(index))"
      ],
      "id": "contemporary-quarter",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tamil-charter"
      },
      "source": [
        "#### OpenMP variables\n",
        "\n",
        "Every parallel context provides its number of threads and the current thread's thread_num in the same way OpenMP does:"
      ],
      "id": "tamil-charter"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "double-hypothesis"
      },
      "source": [
        "with pymp.Parallel(4) as p:\n",
        "    p.print(p.num_threads, p.thread_num)"
      ],
      "id": "double-hypothesis",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lined-indonesia"
      },
      "source": [
        "The original thread entering the parallel context always has `thread_num` 0"
      ],
      "id": "lined-indonesia"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sapphire-metadata"
      },
      "source": [
        "#### Variable scopes\n",
        "\n",
        "The only implemented variable scopes are first private, shared and private.\n",
        "\n",
        "- All variables that are declared before the `pymp.Parallel` call are implicitly first private\n",
        "- All variables from the `pymp.shared` module are shared\n",
        "- All variables created within a `pymp.Parallel` context are private.\n",
        "\n",
        "The package `pymp.shared` provides a numpy array wrapper accepting the standard datatype strings, as well as shared list, dict, queue, lock and rlock objects wrapped from multiprocessing. High performance shared memory (ctypes) data structues are array, lock and rlock, the other data structures are synchronized via a *multiprocessing.Manager* and hence a little slower.\n",
        "\n",
        "All data structures must be synchronized manually, if required, by using a lock. The parallel context offers one for your convenience:"
      ],
      "id": "sapphire-metadata"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "refined-villa"
      },
      "source": [
        "# int array\n",
        "incremental_array = pymp.shared.array((1,), dtype='uint8')\n",
        "print(incremental_array)\n",
        "# list\n",
        "no_of_threads = pymp.shared.list()\n",
        "\n",
        "with pymp.Parallel(4) as p:\n",
        "    for index in p.range(0, 100):\n",
        "        with p.lock:\n",
        "            no_of_threads.append(p.thread_num)\n",
        "            incremental_array[0] += 1\n",
        "print(incremental_array)\n",
        "print(no_of_threads)\n",
        "# check the no.of threads\n",
        "set([i for i in no_of_threads])"
      ],
      "id": "refined-villa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lztkMQ6zMx1d"
      },
      "source": [
        "incremental_array"
      ],
      "id": "lztkMQ6zMx1d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "existing-imaging"
      },
      "source": [
        "#### Nested loops\n",
        "\n",
        "When `pymp.config.nested is True`, it is possible to nest parallel contexts with the expected semantics:\n",
        "\n",
        "**Uncomment the code below and execute the try except block again**"
      ],
      "id": "existing-imaging"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "comfortable-bunny"
      },
      "source": [
        "pymp.config.nested = True"
      ],
      "id": "comfortable-bunny",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "indonesian-practitioner"
      },
      "source": [
        "# nested\n",
        "try:\n",
        "    with pymp.Parallel(4) as p1:\n",
        "        with pymp.Parallel(2) as p2:\n",
        "            p.print(p1.thread_num, p2.thread_num)\n",
        "except:\n",
        "    print(\"Its an Error!\")"
      ],
      "id": "indonesian-practitioner",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "infinite-iraqi"
      },
      "source": [
        "#### Laplace Solver Example\n",
        "\n",
        "The common [Laplace solver](https://www.codeproject.com/Articles/1087025/Using-Python-to-Solve-Computational-Physics-Proble), is a little more detailed. The code is definitely not the most efficient, it uses loops\n",
        "\n",
        "**Note:** Laplace solver is used as an example to calculate the computation time"
      ],
      "id": "infinite-iraqi"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heated-blake"
      },
      "source": [
        "nx = 1201\n",
        "ny = 1201\n",
        "\n",
        "# Solution and previous solution arrays\n",
        "sol = np.zeros((nx,ny))\n",
        "\n",
        "# make a copy of an array\n",
        "soln = sol.copy()\n",
        "\n",
        "for j in range(0,ny-1):\n",
        "    sol[0,j] = 10.0\n",
        "    sol[nx-1,j] = 1.0\n",
        "\n",
        "for i in range(0,nx-1):\n",
        "    sol[i,0] = 0.0\n",
        "    sol[i,ny-1] = 0.0"
      ],
      "id": "heated-blake",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7rznH9EHL1O"
      },
      "source": [
        "# Iterate\n",
        "start_time = time.perf_counter()\n",
        "for kloop in range(1,10):\n",
        "    soln = sol.copy()\n",
        "    for i in range(1,nx-1):\n",
        "        for j in range (1,ny-1):\n",
        "            sol[i,j] = 0.25 * (soln[i,j-1] + soln[i,j+1] + soln[i-1,j] + soln[i+1,j])\n",
        "end_time = time.perf_counter()\n",
        "\n",
        "print('Elapsed wall clock time = %g seconds.' % (end_time-start_time) )"
      ],
      "id": "z7rznH9EHL1O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "outstanding-silence"
      },
      "source": [
        "Same Implementation of laplace solver using Pymp"
      ],
      "id": "outstanding-silence"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rural-protocol"
      },
      "source": [
        "# Solution and previous solution arrays\n",
        "sol = pymp.shared.array((nx,ny))\n",
        "soln = pymp.shared.array((nx,ny))\n",
        "\n",
        "for j in range(0,ny-1):\n",
        "    sol[0,j] = 10.0\n",
        "    sol[nx-1,j] = 1.0\n",
        "\n",
        "for i in range(0,nx-1):\n",
        "    sol[i,0] = 0.0\n",
        "    sol[i,ny-1] = 0.0\n",
        "\n",
        "# Iterate\n",
        "start_time = time.perf_counter()\n",
        "with pymp.Parallel(4) as p:\n",
        "    for kloop in range(1,10):\n",
        "        soln = sol.copy()\n",
        "        for i in p.range(1,nx-1):\n",
        "            for j in p.range (1,ny-1):\n",
        "                sol[i,j] = 0.25 * (soln[i,j-1] + soln[i,j+1] + soln[i-1,j] + soln[i+1,j])\n",
        "\n",
        "end_time = time.perf_counter()\n",
        "print('Elapsed wall clock time = %g seconds.' % (end_time-start_time) )"
      ],
      "id": "rural-protocol",
      "execution_count": null,
      "outputs": []
    }
  ]
}